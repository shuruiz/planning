{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Flatten, Reshape\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D,concatenate\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from attention import Attention_Block\n",
    "# from attention import attention_block\n",
    "from tensorflow.keras.layers import Attention\n",
    "from step_attention import AttentionAugmentation2D, _normalize_depth_vars\n",
    "# from keras.models import load_model\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "from loss import discounted_l1, max_displacement_error,v3_displacement_error\n",
    "import math\n",
    "import pickle5 as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array\n",
    "# import horovod.tensorflow.keras as hvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "checking gpu error\n",
      "checking GPUs\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:05:00.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# hvd.init()\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "print(gpus)\n",
    "\n",
    "\n",
    "if gpus:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu,True)\n",
    "    except:\n",
    "        print('checking gpu error')\n",
    "print('checking GPUs')\n",
    "\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config=tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.visible_device_list= '0,1'\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "config.log_device_placement=True\n",
    "# config.visible_device_list =2\n",
    "sess=tf.compat.v1.Session(config=config)\n",
    "\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import initializers\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from load_data import load_data\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "home = str(Path.home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before normalized k, v 0.2 0.2 (1, 1)\n",
      "after normalized k, v 4 4 (1, 1)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:From /home/lab1/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 10, 128)           67072     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 256)           394240    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 10, 16, 16, 1)     0         \n",
      "=================================================================\n",
      "Total params: 461,312\n",
      "Trainable params: 461,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 13, 13, 1024)      17408     \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 256)         2097408   \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 128)         131200    \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 3, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 2, 32)          8224      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 1, 1, 16)          2064      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 1, 1, 12)          204       \n",
      "_________________________________________________________________\n",
      "attention_augmentation2d (At (None, 1, 1, 4)           2         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 4,391,774\n",
      "Trainable params: 4,391,774\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 10, 2)]           0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 10, 16, 16, 1)     461312    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 10, 32)            4391774   \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 256)               222720    \n",
      "_________________________________________________________________\n",
      "sequential_3 (Sequential)    (None, 2)                 41282     \n",
      "=================================================================\n",
      "Total params: 5,117,088\n",
      "Trainable params: 5,117,088\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "filters=20\n",
    "kernel_size = (3,3)\n",
    "strides = (1,1)\n",
    "depth_k =0.2\n",
    "depth_v =0.2\n",
    "\n",
    "channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "print(\"before normalized k, v\",depth_k,depth_k,strides)\n",
    "depth_k, depth_v = _normalize_depth_vars(depth_k, depth_v, filters)\n",
    "print(\"after normalized k, v\",depth_k,depth_k,strides)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "time_step=10\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "rnn = Sequential()\n",
    "rnn.add(LSTM(128, return_sequences=True, kernel_regularizer='l1', input_shape=(time_step,2)))\n",
    "rnn.add(LSTM(256, return_sequences=True, kernel_regularizer='l1',input_shape=(time_step,128)))\n",
    "rnn.add(Reshape((time_step,16,16,1)))\n",
    "rnn.summary()\n",
    "\n",
    "\n",
    "\n",
    "cnn = Sequential()\n",
    "\n",
    "cnn.add(Conv2D(1024, (4, 4),input_shape=(16, 16, 1)))\n",
    "cnn.add(Conv2D(512, (2, 2)))\n",
    "cnn.add(Conv2D(256, (4, 4)))\n",
    "cnn.add(Conv2D(128, (2, 2)))\n",
    "cnn.add(MaxPooling2D(2,2))\n",
    "cnn.add(Conv2D(64, (2, 2)))\n",
    "cnn.add(Conv2D(32, (2, 2)))\n",
    "cnn.add(Conv2D(16, (2, 2)))\n",
    "\n",
    "#attention block below\n",
    "cnn.add(Conv2D(2 * depth_k + depth_k, (1, 1), strides))\n",
    "cnn.add(AttentionAugmentation2D(depth_k, depth_v, num_heads=4, relative=True))\n",
    "\n",
    "# cnn.add(Dropout(0.5))\n",
    "cnn.add(Flatten())\n",
    "\n",
    " \n",
    "cnn.add(Dense(128))\n",
    "cnn.add(Dense(32))\n",
    "\n",
    "cnn.summary() \n",
    "\n",
    "\n",
    "\n",
    "rnn2=Sequential()\n",
    "rnn2.add(GRU(256, input_shape=(time_step,32)))\n",
    "\n",
    "\n",
    "dense = Sequential()\n",
    "dense.add(Dense(128))\n",
    "dense.add(Dense(64))\n",
    "dense.add(Dense(2)) # Model output\n",
    "\n",
    "main_input = Input(shape=(time_step,2)) \n",
    "model = rnn(main_input)\n",
    "model = TimeDistributed(cnn)(model) \n",
    " # combine timedistributed cnn, attention, and fully connected network with rnn\n",
    "model = rnn2(model)\n",
    "model = dense(model) # add dense\n",
    "final_model = Model(inputs=main_input, outputs=model)\n",
    "final_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_batch = 32\n",
    "\n",
    "n_epoch  = 150\n",
    "\n",
    "\n",
    "# checkpoint_path =home+ \"/repo/models/lyft_cyc2_{epoch}.h5\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "#                                                  save_weights_only=True,\n",
    "#                                                  verbose=1)\n",
    "\n",
    "\n",
    " #(None, n_steps, n_features)\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.5, nesterov=True)\n",
    "rmsprop =tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "\n",
    "\n",
    "ftrl = tf.keras.optimizers.Ftrl(learning_rate=0.01)\n",
    "\n",
    "\n",
    "mtrc = [tf.keras.metrics.MeanAbsoluteError()]\n",
    "# [tf.keras.metrics.CosineSimilarity(axis=1)]\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.1,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.95)\n",
    "\n",
    "adagrad = tf.keras.optimizers.Adagrad(learning_rate=lr_schedule)\n",
    "adadelta = tf.keras.optimizers.Adadelta(learning_rate=lr_schedule)\n",
    "\n",
    "# # to use customized loss, choose from  discounted_l1 or max_displacement_error function as loss parameter \n",
    "# final_model.compile(loss=v3_displacement_error, optimizer=adadelta, metrics=mtrc)\n",
    "# # final_model.load_weights('/home/lab1/repo/models/lyft_cyclist/lyft_cyclist2_larger_1.h5')\n",
    "\n",
    "# history=final_model.fit(X_train, \n",
    "#                         y_train, epochs=n_epoch, \n",
    "#                         batch_size=n_batch, \n",
    "#                         verbose=2, \n",
    "#                         validation_data=(X_val, y_val),\n",
    "#                         callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Evaluate on test data\")\n",
    "# results = final_model.evaluate(X_test, y_test, batch_size=32)\n",
    "# print(\"test loss, test acc:\", results)\n",
    "\n",
    "\n",
    "# _history_path = home+ '/repo/models/cyc2_history.sav'\n",
    "# pickle.dump(history.history, open(_history_path, 'wb'))\n",
    "\n",
    "# _weights_path =  home+ '/repo/models/cyc2_weights.h5'\n",
    "# final_model.save_weights(_weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_model.compile(loss=v3_displacement_error, optimizer=adadelta, metrics=mtrc)\n",
    "\n",
    "# final_model.load_weights('/home/lab1/repo/models/lyft_vehnew_150.h5')\n",
    "# ground_truth = np.load(\"array/ground_truth_prediction_vehicle.npy\",allow_pickle=True) # load data here\n",
    "# samples = np.load(\"array/history_vehicle.npy\",allow_pickle=True)\n",
    "\n",
    "# final_model.load_weights('/home/lab1/repo/models/lyft_pednew_150.h5')\n",
    "# ground_truth = np.load(\"array/ground_truth_prediction_pedestrian.npy\",allow_pickle=True) # load data here\n",
    "# samples = np.load(\"array/history_pedestrian.npy\",allow_pickle=True)\n",
    "\n",
    "\n",
    "# final_model.load_weights('/home/lab1/repo/models/lyft_cycnew005_150.h5')\n",
    "final_model.load_weights('/home/lab1/repo/models/lyft_cyctransferped_150.h5') # transfer learning from ped\n",
    "# final_model.load_weights('/home/lab1/repo/models/lyft_cyc_transferveh_150.h5') # transfer learning from veh\n",
    "ground_truth = np.load(\"array/ground_truth_prediction_cyclist.npy\",allow_pickle=True) # load data here\n",
    "samples = np.load(\"array/history_cyclist.npy\",allow_pickle=True)\n",
    "\n",
    "\n",
    "##prediction\n",
    "# from utilities import get_position_sequence\n",
    "def get_position_sequence(prev, new):\n",
    "    arr = np.delete(prev, 0, 0)\n",
    "    a=np.vstack((arr,new))\n",
    "    return a \n",
    "\n",
    "def get_sequence_prediction(X_position_test, i):\n",
    "    results =[]\n",
    "    s=X_position_test[i]\n",
    "    for count in range(0,10):\n",
    "        \n",
    "        result = final_model.predict(np.array([s]))\n",
    "#         print(result)\n",
    "        results.append(result[0])\n",
    "        s = get_position_sequence(s, result)\n",
    "    results=np.array(results)\n",
    "#     print(s.shape,results.shape)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# i= np.random.randint(0, len(X_test))\n",
    "# results = get_sequence_prediction(X_test, i)\n",
    "# print(results.shape)\n",
    "\n",
    "# test=np.load('cyclist_test_history.npy')\n",
    "# v_predict=[]\n",
    "# print(len(test))\n",
    "# for i in range(len(test)):\n",
    "#     print(i)\n",
    "#     results = get_sequence_prediction(test, i)\n",
    "#     v_predict.append(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.array(v_predict).shape)\n",
    "# np.save(\"c_predictions\",v_predict)\n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164, 10, 2) (164, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "samples=samples.astype(float)[:3000]\n",
    "ground_truth = ground_truth.astype(float)[:3000]\n",
    "# exception = set(exception.astype(float))\n",
    "# samples= np.around(samples,decimals=6)\n",
    "# ground_truth = np.around(ground_truth,decimals=6)\n",
    "exception = set()\n",
    "\n",
    "print(samples.shape, ground_truth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "Final displacement error: 3.743121159222175\n",
      "Average displacement error: 1.4308402563846883\n"
     ]
    }
   ],
   "source": [
    "final_results=[]\n",
    "final_displacement=[]\n",
    "avg_displacement=[]\n",
    "for idx in range(0,len(samples)):\n",
    "    if idx in exception: \n",
    "        print(idx) \n",
    "        continue\n",
    "    if idx%100==0:print(idx)\n",
    "    p=[]\n",
    "    re= get_sequence_prediction(samples, idx)\n",
    "    final_results.append(re)\n",
    "#     print(re.shape, ground_truth[idx].shape)\n",
    "#     print(re,ground_truth[idx] )\n",
    "    distance = v3_displacement_error(re,ground_truth[idx] )\n",
    "    avg = K.mean(distance)\n",
    "    \n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        p=distance.eval()\n",
    "        ax =avg.eval()\n",
    "#     print(p,ax)\n",
    "    final_displacement.append(p)\n",
    "    avg_displacement.append(ax)\n",
    "\n",
    "final_displacement = np.array(final_displacement)\n",
    "sec=[]\n",
    "\n",
    "for j in range(0,final_displacement.shape[1]):\n",
    "    displacement = sum(final_displacement[:,j])/len(final_displacement)\n",
    "    \n",
    "    sec.append(displacement)\n",
    "print(\"Final displacement error:\",sec[-1])\n",
    "print(\"Average displacement error:\",sum(avg_displacement)/len(avg_displacement))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1280403699476567, 0.3010705987043603, 0.49245525886989894, 0.7115947514986172, 0.9542853649618458, 1.2521581748050092, 1.6496019286759505, 2.1866272567800835, 2.8894477003812877, 3.743121159222175]\n"
     ]
    }
   ],
   "source": [
    "np.save(\"model_cyc_ped\",sec)\n",
    "print(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164, 10, 2) (164, 10, 2)\n",
      "(164, 10)\n"
     ]
    }
   ],
   "source": [
    "final_results=np.array(final_results)\n",
    "# np.save(\"vehicle_prediction_scene_16220\", final_results)\n",
    "print(final_results.shape, ground_truth.shape)\n",
    "print(final_displacement.shape)\n",
    "np.save(\"model_cyctransfer_ped_displacement\",final_displacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164, 10, 2) (164, 10, 2)\n",
      "1.431 3.743\n"
     ]
    }
   ],
   "source": [
    "def compare(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    compute the max displacement in prediction and ground truth\n",
    "    return a loss keras tensor\n",
    "    \"\"\"\n",
    "  \n",
    "    print(y_true.shape, y_pred.shape)\n",
    "    res=[]\n",
    "    avg=[]\n",
    "    for index in range(len(y_true)):\n",
    "        true = y_true[index]\n",
    "        pred= y_pred[index]\n",
    "        error=[]\n",
    "        for p1, p2 in zip(pred, true): # 50 points\n",
    "            d = math.sqrt((p1[0]-p2[0])**2+(p1[1]-p2[1])**2)\n",
    "            error.append(d)\n",
    "\n",
    "        avg.append(sum(error)/len(error))\n",
    "        res.append(error)\n",
    "\n",
    "\n",
    "    res=np.array(res)\n",
    "    ade =round(sum(avg)/len(avg),3)\n",
    "    fde = round(sum(res[:,-1])/len(res),3)\n",
    "\n",
    "    return ade, fde\n",
    "\n",
    "y_true=[]\n",
    "for idx in range(len(ground_truth)):\n",
    "    if idx in exception:\n",
    "        continue\n",
    "    y_true.append(ground_truth[idx])\n",
    "y_true=np.array(y_true)\n",
    "\n",
    "ade, fde = compare(final_results, y_true)\n",
    "print(ade, fde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # i= np.random.randint(0, len(X_test))\n",
    "\n",
    "\n",
    "# # # In[ ]:\n",
    "# # # i =2273\n",
    "\n",
    "# # # i=1574\n",
    "# # # i=3070\n",
    "\n",
    "# # # i=387,39\n",
    "# # # i=667\n",
    "# # # i=15\n",
    "# # print(\"showing trip\", i)\n",
    "\n",
    "\n",
    "# # from utilities import get_position_sequence\n",
    "# def get_position_sequence(prev, new):\n",
    "#     arr = np.delete(prev, 0, 0)\n",
    "#     a=np.vstack((arr,new))\n",
    "#     return a \n",
    "\n",
    "# def get_sequence_prediction(X_position_test, i):\n",
    "#     results =[]\n",
    "#     s=X_position_test[i]\n",
    "#     for count in range(0,12):\n",
    "        \n",
    "#         result = final_model.predict(np.array([s]))\n",
    "# #         print(result)\n",
    "#         results.append(result[0])\n",
    "#         s = get_position_sequence(s, result)\n",
    "#     results=np.array(results)\n",
    "# #     print(s.shape,results.shape)\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# results = get_sequence_prediction(X_test, i)\n",
    "\n",
    "# # print(len(results))\n",
    "# # print(\"sss\",results)\n",
    "\n",
    "# ax1 =plt.scatter(y_test[i][:,0],y_test[i][:,1],color='b',label='Ground truth')\n",
    "\n",
    "# ax2 =plt.scatter(X_test[i][:,0],X_test[i][:,1],color='r', label='Training segment')\n",
    "\n",
    "\n",
    "# ax3 =plt.scatter(results[:,0],results[:,1],color='g', label='Prediction')\n",
    "# # ax3 =plt.scatter(result[0][0],result[0][1],color='g', label='Prediction')\n",
    "\n",
    "# plt.legend(handles=[ax1, ax2, ax3])\n",
    "# plt.show()\n",
    "\n",
    "# distance = v3_displacement_error(results,y_test[i] )\n",
    "# avg = K.mean(distance)\n",
    "# pl =[]\n",
    "# # tf.print(distance,output_stream=sys.stderr)\n",
    "# with tf.compat.v1.Session() as sess: pl=distance.eval(), print(distance.eval(),'\\naverage',avg.eval()) \n",
    "# import matplotlib.pyplot as plt\n",
    "# # print(pl[0])\n",
    "# plt.plot(pl[0])\n",
    "# plt.title('Displacement error')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_displacement=[]\n",
    "# avg_displacement=[]\n",
    "\n",
    "# # for idx in range(0,len(X_test)):\n",
    "# for idx in range(0,1000):\n",
    "#     p=[]\n",
    "#     re= get_sequence_prediction(X_test, idx)\n",
    "#     distance = v3_displacement_error(re,y_test[idx] )\n",
    "#     avg = K.mean(distance)\n",
    "    \n",
    "#     with tf.compat.v1.Session() as sess:\n",
    "#         p=distance.eval()\n",
    "#         ax =avg.eval()\n",
    "# #     print(p,ax)\n",
    "#     final_displacement.append(p)\n",
    "#     avg_displacement.append(ax)\n",
    "\n",
    "# final_displacement = np.array(final_displacement)\n",
    "# sec=[]\n",
    "\n",
    "# for j in range(0,final_displacement.shape[1]):\n",
    "#     displacement = sum(final_displacement[:,j])/len(final_displacement)\n",
    "    \n",
    "#     sec.append(displacement)\n",
    "# print(\"Final displacement error:\",sec[-1])\n",
    "# print(\"Average displacement error:\",sum(avg_displacement)/len(avg_displacement))\n",
    "\n",
    "# print(sec)\n",
    "# plt.plot(sec)\n",
    "# plt.title(\"average displacement error at each timestep\")\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history2 = pickle.load(open( cwd+ '/saved_model/lab1_txt_max_history_mae_adadelta_600.sav',  \"rb\") ) \n",
    "# history3 = pickle.load(open( cwd+ '/saved_model/lab1_txt_max_history_mae_adadelta.sav',  \"rb\") )\n",
    "# print(len(history.history['loss']))\n",
    "# # plt.plot(range(600,1100),history.history['loss'], c='b')\n",
    "# # plt.plot( range(300,600), history2['loss'], c='b')\n",
    "# plt.plot( history3['loss'], c='b')\n",
    "# plt.title('train loss')\n",
    "\n",
    "# # plt.plot(range(600,1100),history.history['val_loss'], c='r')\n",
    "# # plt.plot( range(300,600), history2['val_loss'], c='r')\n",
    "# plt.plot( history3['val_loss'], c='r')\n",
    "# plt.title('loss')\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
