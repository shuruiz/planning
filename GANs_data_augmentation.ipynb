{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Flatten, Reshape\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D,concatenate\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import initializers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import math\n",
    "import pickle5 as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "checking gpu error\n",
      "checking GPUs\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:06:00.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu,True)\n",
    "    except:\n",
    "        print('checking gpu error')\n",
    "print('checking GPUs')\n",
    "\n",
    "config=tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.visible_device_list= '0,1'\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "config.log_device_placement=True\n",
    "# config.visible_device_list =2\n",
    "sess=tf.compat.v1.Session(config=config)\n",
    "\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data setting\n",
    "sample_size = 10000\n",
    "num_per_channel=3\n",
    "x_column=150\n",
    "y_column=150\n",
    "channels=40\n",
    "sample_shape= (num_per_channel, channels, x_column, y_column, )\n",
    "z_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(img_shape, z_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=z_dim))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dense(num_per_channel*x_column*y_column*channels, activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "    z = Input(shape=(z_dim,))\n",
    "    img = model(z)\n",
    "    return Model(z, img)\n",
    "\n",
    "def discriminator(img_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    img = Input(shape=img_shape)\n",
    "    prediction = model(img)\n",
    "    return Model(img, prediction)\n",
    "\n",
    "discriminator = discriminator(sample_shape)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "generator = generator(sample_shape, z_dim)\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "discriminator.trainable = False\n",
    "prediction = discriminator(img)\n",
    "combined = Model(z, prediction)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "def train(iterations, batch_size, sample_interval,xtrain, xtest):\n",
    "    gen_images = []\n",
    "    \n",
    "    # Assign X_train to X_train_0 for augment non-cactus images\n",
    "    # Assign X_train to X_train_1 for augment cactus images\n",
    "\n",
    "    X_train = np.array(xtrain)\n",
    "    X_test = np.array(xtest)\n",
    "    real = np.ones((batch_size, 1))\n",
    "#     real = np.random.choice(X_test, size=batch_size, replace=True, p=None)\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "       \n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        z = np.random.normal(0, 1, (batch_size, 100))\n",
    "        gen_imgs = generator.predict(z)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, real)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        z = np.random.normal(0, 1, (batch_size, 100))\n",
    "        gen_imgs = generator.predict(z)\n",
    "        g_loss = combined.train_on_batch(z, real)\n",
    "\n",
    "        if iteration % sample_interval == 0:\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (iteration, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            losses.append((d_loss[0], g_loss))\n",
    "            accuracies.append(100*d_loss[1])\n",
    "            gen_images.append(sample_images(iteration))\n",
    "    return gen_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sample_images(iteration, image_grid_rows=4, image_grid_columns=4):\n",
    "\n",
    "    z = np.random.normal(0, 1, \n",
    "              (image_grid_rows * image_grid_columns, z_dim))\n",
    "\n",
    "    gen_imgs = generator.predict(z)\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "            \n",
    "    return gen_imgs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 3, 40, 150, 150)\n",
      "0 [D loss: 0.700286, acc.: 21.09%] [G loss: 39.387867]\n",
      "5 [D loss: 0.162964, acc.: 99.22%] [G loss: 888.378052]\n",
      "10 [D loss: 447.767141, acc.: 46.09%] [G loss: 0.000000]\n",
      "15 [D loss: 0.300565, acc.: 78.12%] [G loss: 4675.812012]\n",
      "20 [D loss: 0.333661, acc.: 71.09%] [G loss: 3052.625732]\n",
      "25 [D loss: 82.263631, acc.: 25.00%] [G loss: 4319.197266]\n",
      "30 [D loss: 7.137455, acc.: 53.12%] [G loss: 6916.749023]\n",
      "35 [D loss: 5.679444, acc.: 62.50%] [G loss: 7962.665527]\n",
      "40 [D loss: 6.303889, acc.: 64.84%] [G loss: 9690.437500]\n",
      "45 [D loss: 3.894343, acc.: 70.31%] [G loss: 6540.612305]\n",
      "50 [D loss: 3.568990, acc.: 78.91%] [G loss: 2330.510986]\n",
      "55 [D loss: 0.833726, acc.: 86.72%] [G loss: 7000.575195]\n",
      "60 [D loss: 30.859717, acc.: 85.16%] [G loss: 2386.998779]\n",
      "65 [D loss: 0.593394, acc.: 93.75%] [G loss: 2407.344238]\n",
      "70 [D loss: 0.165227, acc.: 96.88%] [G loss: 8833.723633]\n",
      "75 [D loss: 0.127205, acc.: 98.44%] [G loss: 9293.695312]\n",
      "80 [D loss: 2.552987, acc.: 98.44%] [G loss: 2144.006592]\n",
      "85 [D loss: 0.091289, acc.: 97.66%] [G loss: 15734.078125]\n",
      "90 [D loss: 0.054388, acc.: 100.00%] [G loss: 14370.129883]\n",
      "95 [D loss: 0.078989, acc.: 99.22%] [G loss: 6762.166016]\n",
      "100 [D loss: 0.051742, acc.: 100.00%] [G loss: 5056.040039]\n",
      "105 [D loss: 1.589196, acc.: 98.44%] [G loss: 4156.908203]\n",
      "110 [D loss: 0.072969, acc.: 100.00%] [G loss: 4715.669434]\n",
      "115 [D loss: 10.116037, acc.: 98.44%] [G loss: 3850.964844]\n",
      "120 [D loss: 1.113729, acc.: 99.22%] [G loss: 5307.605469]\n",
      "125 [D loss: 7.185375, acc.: 99.22%] [G loss: 4943.556641]\n",
      "130 [D loss: 29.412644, acc.: 96.88%] [G loss: 3153.026855]\n",
      "135 [D loss: 1.581899, acc.: 98.44%] [G loss: 4512.145508]\n",
      "140 [D loss: 14.685222, acc.: 97.66%] [G loss: 4406.510742]\n",
      "145 [D loss: 4.612196, acc.: 99.22%] [G loss: 4618.313477]\n",
      "150 [D loss: 8.570106, acc.: 96.88%] [G loss: 3995.441406]\n",
      "155 [D loss: 0.046788, acc.: 100.00%] [G loss: 3692.589355]\n",
      "160 [D loss: 14.728289, acc.: 96.88%] [G loss: 3221.331543]\n",
      "165 [D loss: 0.069153, acc.: 100.00%] [G loss: 2735.910645]\n",
      "170 [D loss: 0.041516, acc.: 100.00%] [G loss: 3348.217041]\n",
      "175 [D loss: 0.045830, acc.: 100.00%] [G loss: 2964.188721]\n",
      "180 [D loss: 0.032482, acc.: 100.00%] [G loss: 2366.407227]\n",
      "185 [D loss: 0.040444, acc.: 100.00%] [G loss: 2410.172852]\n",
      "190 [D loss: 0.049446, acc.: 100.00%] [G loss: 2095.363037]\n",
      "195 [D loss: 0.031897, acc.: 100.00%] [G loss: 1413.928467]\n",
      "200 [D loss: 0.049335, acc.: 100.00%] [G loss: 1197.763184]\n",
      "205 [D loss: 0.037680, acc.: 100.00%] [G loss: 885.172729]\n",
      "210 [D loss: 0.032275, acc.: 100.00%] [G loss: 661.456055]\n",
      "215 [D loss: 0.037440, acc.: 100.00%] [G loss: 405.965881]\n",
      "220 [D loss: 0.121379, acc.: 97.66%] [G loss: 248.735138]\n",
      "225 [D loss: 0.023904, acc.: 100.00%] [G loss: 455.387024]\n",
      "230 [D loss: 0.034307, acc.: 100.00%] [G loss: 305.898560]\n",
      "235 [D loss: 0.035650, acc.: 100.00%] [G loss: 180.430283]\n",
      "240 [D loss: 0.169214, acc.: 99.22%] [G loss: 253.912430]\n",
      "245 [D loss: 0.034325, acc.: 100.00%] [G loss: 332.776306]\n",
      "250 [D loss: 0.025863, acc.: 100.00%] [G loss: 245.826782]\n",
      "255 [D loss: 0.100208, acc.: 99.22%] [G loss: 247.773712]\n",
      "260 [D loss: 0.015105, acc.: 100.00%] [G loss: 272.203064]\n",
      "265 [D loss: 0.035660, acc.: 100.00%] [G loss: 152.364136]\n",
      "270 [D loss: 0.017107, acc.: 100.00%] [G loss: 233.091049]\n",
      "275 [D loss: 0.034944, acc.: 100.00%] [G loss: 233.780243]\n",
      "280 [D loss: 0.631078, acc.: 97.66%] [G loss: 187.279938]\n",
      "285 [D loss: 0.014606, acc.: 100.00%] [G loss: 328.433167]\n",
      "290 [D loss: 0.026939, acc.: 100.00%] [G loss: 287.187195]\n",
      "295 [D loss: 0.019092, acc.: 100.00%] [G loss: 243.022888]\n",
      "300 [D loss: 0.027912, acc.: 100.00%] [G loss: 154.202621]\n",
      "305 [D loss: 0.063469, acc.: 99.22%] [G loss: 151.413513]\n",
      "310 [D loss: 0.014213, acc.: 100.00%] [G loss: 161.604919]\n",
      "315 [D loss: 0.009297, acc.: 100.00%] [G loss: 206.767059]\n",
      "320 [D loss: 0.016757, acc.: 100.00%] [G loss: 295.592163]\n",
      "325 [D loss: 0.022429, acc.: 100.00%] [G loss: 223.315231]\n",
      "330 [D loss: 0.012519, acc.: 100.00%] [G loss: 207.742676]\n",
      "335 [D loss: 0.082934, acc.: 99.22%] [G loss: 154.516937]\n",
      "340 [D loss: 0.007268, acc.: 100.00%] [G loss: 156.592651]\n",
      "345 [D loss: 0.057368, acc.: 99.22%] [G loss: 126.648407]\n",
      "350 [D loss: 0.016537, acc.: 100.00%] [G loss: 111.629005]\n",
      "355 [D loss: 0.030396, acc.: 100.00%] [G loss: 92.407852]\n",
      "360 [D loss: 0.009097, acc.: 100.00%] [G loss: 148.905304]\n",
      "365 [D loss: 0.019679, acc.: 100.00%] [G loss: 210.871613]\n",
      "370 [D loss: 0.026934, acc.: 100.00%] [G loss: 186.974640]\n",
      "375 [D loss: 0.005921, acc.: 100.00%] [G loss: 139.133118]\n",
      "380 [D loss: 0.026212, acc.: 100.00%] [G loss: 126.297264]\n",
      "385 [D loss: 0.116079, acc.: 99.22%] [G loss: 82.298203]\n",
      "390 [D loss: 0.005711, acc.: 100.00%] [G loss: 116.645012]\n",
      "395 [D loss: 0.004593, acc.: 100.00%] [G loss: 107.438698]\n",
      "400 [D loss: 0.019096, acc.: 100.00%] [G loss: 110.249023]\n",
      "405 [D loss: 0.004070, acc.: 100.00%] [G loss: 99.545616]\n",
      "410 [D loss: 0.003820, acc.: 100.00%] [G loss: 89.088791]\n",
      "415 [D loss: 0.013276, acc.: 100.00%] [G loss: 73.023407]\n",
      "420 [D loss: 0.011808, acc.: 100.00%] [G loss: 67.261612]\n",
      "425 [D loss: 0.011857, acc.: 100.00%] [G loss: 67.816360]\n"
     ]
    }
   ],
   "source": [
    "# Set iterations at least 10000 for good results\n",
    "iterations = 5000\n",
    "batch_size = 64\n",
    "sample_interval = 5\n",
    "xtrain = np.load('8kfb_representation.npy', allow_pickle=True)\n",
    "print(xtrain.shape)\n",
    "np.random.shuffle(xtrain)\n",
    "split = math.floor(len(xtrain)*0.9)\n",
    "xtrain, xtest = xtrain[:split], xtrain[split:]\n",
    "gen_imgs = train(iterations, batch_size, sample_interval, xtrain,xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.normal(0, 1, (batch_size, 100))\n",
    "output = generator.predict(z)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replot from representation\n",
    "from collections import defaultdict\n",
    "def find_traj(tensor):\n",
    "#     print(tensor.shape)\n",
    "    result_traj=defaultdict(list) #{id:traj}, traj is 2D\n",
    "    for snapshot in tensor:\n",
    "        max_v = int(np.max(snapshot))\n",
    "        if max_v==0:\n",
    "            continue\n",
    "        for value in range(1, max_v+1):\n",
    "#             print(value)\n",
    "            loc = np.where(snapshot==value)\n",
    "            if loc[0].size==0:\n",
    "                continue\n",
    "            result_traj[value].append([loc[0][0]+450, loc[1][0]-2450])\n",
    "    return result_traj\n",
    "            \n",
    "    \n",
    "def replot_from_representation(scene_tensor, lane_list, intersection_id, grid_boundary):\n",
    "\n",
    "    veh, ped, cyc = scene_tensor[0], scene_tensor[1], scene_tensor[2]\n",
    "    veh_traj = find_traj(veh)\n",
    "    ped_traj = find_traj(ped)\n",
    "    cyc_traj = find_traj(cyc)\n",
    "    \n",
    "    plt.figure(figsize=(18,18))\n",
    "    for lane in lane_list[intersection_id]:\n",
    "        plt.plot(Test.map_api.get_lane_coords(lane)['xyz_right'][:,0], Test.map_api.get_lane_coords(lane)['xyz_right'][:,1],\n",
    "                 color='g',linewidth=1,label=lane)\n",
    "        plt.plot(Test.map_api.get_lane_coords(lane)['xyz_left'][:,0], Test.map_api.get_lane_coords(lane)['xyz_left'][:,1],\n",
    "                 color='g',linewidth=1)\n",
    "#         print(\"lane coordinates right\", Test.map_api.get_lane_coords(lane)['xyz_right'][:,0])\n",
    "    x_mesh, y_mesh = get_grid(intersection_id, grid_boundary)\n",
    "#     print(x_mesh.shape, y_mesh[:,1])\n",
    "    plt.plot(x_mesh, y_mesh, c='grey', linewidth=0.1) # use plot, not scatter\n",
    "    plt.plot(np.transpose(x_mesh), np.transpose(y_mesh),c='grey', linewidth=0.1) # add this here\n",
    "#     plt.title(intersection_id,fontsize=30)\n",
    "    \n",
    "    for key, traj in veh_traj.items():\n",
    "        traj = np.array(traj)\n",
    "        plt.plot(traj[:,0],traj[:,1], c='b',linewidth=0.5)\n",
    "    for key, traj in ped_traj.items():\n",
    "        traj = np.array(traj)\n",
    "        plt.plot(traj[:,0],traj[:,1], c='orange',linewidth=0.5)\n",
    "    for key, traj in cyc_traj.items():\n",
    "        traj = np.array(traj)\n",
    "        plt.plot(traj[:,0],traj[:,1], c='cyan',linewidth=0.5)\n",
    "#     if veh is not None:\n",
    "#         for traj in veh: plt.plot(traj[:,0],traj[:,1], c='b',linewidth=0.5)\n",
    "#     if ped is not None:\n",
    "#         for traj in ped: plt.scatter(traj[:,0],traj[:,1], c='orange',linewidth=0.5)\n",
    "#     if cyc is not None:\n",
    "#         for traj in cyc: plt.plot(traj[:,0],traj[:,1], c='cyan',linewidth=0.5)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "# locate to the working folder\n",
    "# sys.path.append(\"D:\\\\GitHub\\\\Clone\\\\planning\\\\ruixuan\")\n",
    "from ruixuan.turning_scene import *\n",
    "from l5kit.rasterization.rasterizer_builder import _load_metadata\n",
    "import time\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"/home/lab1/repo/planning/prediction-dataset\"\n",
    "# get config\n",
    "cfg = load_config_data(\"/home/lab1/repo/planning/ruixuan/visualisation_config.yaml\")\n",
    "print(cfg)\n",
    "\n",
    "dm = LocalDataManager()\n",
    "dataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\n",
    "zarr_dataset = ChunkedDataset(dataset_path)\n",
    "zarr_dataset.open()\n",
    "print(zarr_dataset)\n",
    "\n",
    "rast = build_rasterizer(cfg, dm)\n",
    "ego_dataset = EgoDataset(cfg, zarr_dataset, rast)\n",
    "# agent_dataset = AgentDataset(cfg, zarr_dataset, rast)\n",
    "# Obatin the information from semantic map\n",
    "\n",
    "semantic_map_filepath = dm.require(cfg[\"raster_params\"][\"semantic_map_key\"])\n",
    "dataset_meta = _load_metadata(cfg[\"raster_params\"][\"dataset_meta_key\"], dm)\n",
    "world_to_ecef = np.array(dataset_meta[\"world_to_ecef\"], dtype=np.float64)\n",
    "Map_Api = MapAPI(semantic_map_filepath, world_to_ecef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manully find the lane id in Map_Api for \"lane_merge\" and \"8KfB\"\n",
    "lane_list = {}\n",
    "junction_boundary = {}\n",
    "# extend lane sequences\n",
    "lane_list['lane_merge'] = ['ADrl',\"oFEC\",'m0JU','iQgg','M5V5','/Pgg','FFEC','XHTU']\n",
    "lane_list['8KfB'] = ['/24B','6p63','FV1O','MV/U','SxVb','TG2b','TZZv','ZnUV','bH1o','dddQ','nXc0','zHjP','SD8o','vC8o']\n",
    "junction_boundary['lane_merge'] = [(-940,1380),(-940,1480),(-880,1380),(-880,1480)]\n",
    "junction_boundary['8KfB'] = [(500,-2420),(500,-2360),(560,-2360),(560,-2420)]\n",
    "# junction_boundary['8KfB'] = [(450,-2400),(500,-2360),(560,-2360),(560,-2420)]\n",
    "\n",
    "grid_boundary={}\n",
    "grid_boundary['8KfB']={'X':[450,600], 'Y':[-2450, -2300]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory(Scene):\n",
    "    \n",
    "    def __init__(self, dataset, Map_Api):\n",
    "        super(Trajectory, self).__init__(dataset, Map_Api)\n",
    "        self.label_name = ['Car','Van','Tram','Bus','Truck','EV','OV','Bicycle',\\\n",
    "                           'Motorcycle','Cyclist','Motorcyclist','Pedestrian']\n",
    "        self.label_idx = list(range(3,15))\n",
    "        self.label_dict = dict(zip(self.label_name, self.label_idx))\n",
    "        self.all_traffic_control = None\n",
    "        self.Traffic_Control = None\n",
    "\n",
    "    def generate_info_from_MAP(self):\n",
    "        self.all_junctions = self.get_elements(\"junction\")\n",
    "        self.all_lanes = self.get_elements(\"lane\")\n",
    "        self.all_traffic_control = self.get_elements(\"traffic_control_element\")\n",
    "        self.Lane = {self.map_api.id_as_str(lane.id):lane.element.lane for lane in self.all_lanes}\n",
    "        self.Junction = {self.map_api.id_as_str(junction.id):junction for junction in self.all_junctions}\n",
    "        self.Traffic_Control = {self.map_api.id_as_str(traffic_control.id):traffic_control.element for traffic_control in self.all_traffic_control}\n",
    "        \n",
    "        for junction in self.all_junctions:\n",
    "            self.Junction_Lane[self.map_api.id_as_str(junction.id)] = []\n",
    "\n",
    "            for lane in junction.element.junction.lanes:\n",
    "                self.Junction_Lane[self.map_api.id_as_str(junction.id)].append(self.map_api.id_as_str(lane))\n",
    "\n",
    "            self.Junction_Lane[self.map_api.id_as_str(junction.id)] = set(self.Junction_Lane[self.map_api.id_as_str(junction.id)])\n",
    "        \n",
    "        self.junction_scene = dict.fromkeys(list(self.Junction.keys()), [])\n",
    "        self.junction_turning_scene = dict.fromkeys(list(self.Junction.keys()), {})\n",
    "        for key in self.Junction.keys():\n",
    "            self.junction_turning_scene[key] = {'Turning Left': [], 'Turning Right': []}    \n",
    "        \n",
    "        \n",
    "    def label_loc_check(self, target_label):\n",
    "    \n",
    "        agent_id_list = self.agent_list[self.label_dict[target_label]-3]\n",
    "\n",
    "        for agent_id in agent_id_list:\n",
    "            agent_loc = self.agent_centroid[np.where(self.agent_id==agent_id)[0]]\n",
    "            for centroid in agent_loc:\n",
    "                if self.Junction_region.contains(Point(centroid[0],centroid[1])):\n",
    "                    return True\n",
    "    \n",
    "    def junction_lane_visualize(self, junction_id):\n",
    "        \n",
    "        plt.figure(figsize=(18,18))\n",
    "        \n",
    "        lane_list = self.Junction_Lane[junction_id]\n",
    "        \n",
    "        for lane in lane_list:\n",
    "            plt.plot(self.map_api.get_lane_coords(lane)['xyz_left'][:,0].tolist()+ self.map_api.get_lane_coords(lane)['xyz_right'][:,0].tolist(),\n",
    "                        self.map_api.get_lane_coords(lane)['xyz_left'][:,1].tolist()+ self.map_api.get_lane_coords(lane)['xyz_right'][:,1].tolist(),\n",
    "                        marker='x', label = lane)\n",
    "\n",
    "        plt.axis(\"equal\")\n",
    "        plt.grid(which='both')\n",
    "        plt.legend(fontsize=20)\n",
    "        axes = plt.gca()  \n",
    "    \n",
    "                \n",
    "    def agent_trajectory(self, scene, target_label, junction_boundary):\n",
    "\n",
    "        self.scene = scene\n",
    "        self.target_label = target_label\n",
    "#         self.junction = junction\n",
    "        # region for junction \"sGK1\"\n",
    "#         junction_boundary = {junction:[(300,-1150),[300,-1100],(340,-1100),(340,-1150)]}\n",
    "\n",
    "        # region for junction \"8KfB\"\n",
    "#         junction_boundary = {junction:[(500,-2420),(500,-2360),(560,-2360),(560,-2420)]}\n",
    "        \n",
    "        # region for lane merge\n",
    "#         junction_boundary = {junction:[(-940,1380),(-940,1480),(-880,1380),(-880,1480)]}\n",
    "\n",
    "        self.Junction_region = Polygon(junction_boundary)\n",
    "\n",
    "        self.agent_list = []\n",
    "        frame_interval = self.dataset.scenes[scene]['frame_index_interval']\n",
    "        agent_interval_begin = self.dataset.frames[frame_interval[0]]['agent_index_interval'][0]\n",
    "        agent_interval_end = self.dataset.frames[frame_interval[1]-1]['agent_index_interval'][1]\n",
    "        self.agent_id = self.dataset.agents[agent_interval_begin:agent_interval_end]['track_id']\n",
    "        agent_label_prob = self.dataset.agents[agent_interval_begin:agent_interval_end]['label_probabilities']\n",
    "        self.agent_centroid = self.dataset.agents[agent_interval_begin:agent_interval_end]['centroid']\n",
    "\n",
    "        for label in self.label_idx:\n",
    "            valid_idx = np.where(agent_label_prob[:,label]>0.5)[0]\n",
    "            valid_id = set(self.agent_id[valid_idx])\n",
    "            self.agent_list.append(list(valid_id))\n",
    "\n",
    "        if len(self.agent_list[self.label_dict[target_label]-3])>0 and self.label_loc_check(target_label):\n",
    "            return [scene]\n",
    "        else:\n",
    "            # print('Not feasible scene')\n",
    "            return []\n",
    "        \n",
    "    def get_agent_traj(self, scene, target_label, junction, lane_list):\n",
    "        veh_traj, ped_traj, cyclist_traj=[], [], [] \n",
    "        print(self.label_dict, self.label_dict[target_label])\n",
    "        for idx, agent_label in enumerate(self.agent_list):\n",
    "            print(\"xxx\", idx, agent_label)\n",
    "            for agent in agent_label:\n",
    "                agent_loc = self.agent_centroid[np.where(self.agent_id==agent)[0]]\n",
    "                if idx != self.label_dict[target_label]-3 and agent_loc.shape[0] > 30  \\\n",
    "                and np.linalg.norm(agent_loc[0,:]-agent_loc[-1,:]) > 5:\n",
    "                    veh_traj.append(agent_loc)\n",
    "#                     plt.plot(agent_loc[:,0],agent_loc[:,1],label='Agent ID '+str(agent)+': '\\\n",
    "#                              +self.label_name[idx], color=colors[idx])\n",
    "\n",
    "                elif idx== self.label_dict[target_label]-3:\n",
    "                    ped_traj.append(agent_loc)\n",
    "        veh_traj, ped_traj, cyclist_traj = np.array(veh_traj), np.array(ped_traj), np.array(cyclist_traj)\n",
    "        return veh_traj, ped_traj,cyclist_traj\n",
    "#                     plt.plot(agent_loc[:,0],agent_loc[:,1], color=colors[idx], marker='*')\n",
    "            \n",
    "            \n",
    "            \n",
    "    def trajectory_junction_visualize(self, scene, target_label, junction, lane_list):\n",
    "        \n",
    "        frame_interval = self.dataset.scenes[scene]['frame_index_interval']\n",
    "        \n",
    "        plt.figure(figsize=(18,18))\n",
    "\n",
    "        for lane in lane_list:\n",
    "            plt.plot(self.map_api.get_lane_coords(lane)['xyz_right'][:,0], self.map_api.get_lane_coords(lane)['xyz_right'][:,1],\n",
    "                     color='k',linewidth=5, alpha=0.2)\n",
    "            plt.plot(self.map_api.get_lane_coords(lane)['xyz_left'][:,0], self.map_api.get_lane_coords(lane)['xyz_left'][:,1],\n",
    "                     color='k',linewidth=5, alpha=0.2)\n",
    "\n",
    "        cmap = plt.get_cmap('gnuplot')    \n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "         '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf','maple','yellowgreen'] \n",
    "\n",
    "        for idx, agent_label in enumerate(self.agent_list):\n",
    "            for agent in agent_label:\n",
    "                agent_loc = self.agent_centroid[np.where(self.agent_id==agent)[0]]\n",
    "                if idx != self.label_dict[target_label]-3 and agent_loc.shape[0] > 30  and np.linalg.norm(agent_loc[0,:]-agent_loc[-1,:]) > 5:\n",
    "                    plt.plot(agent_loc[:,0],agent_loc[:,1],label='Agent ID '+str(agent)+': '+self.label_name[idx], color=colors[idx])\n",
    "\n",
    "                elif idx== self.label_dict[target_label]-3:\n",
    "                    plt.plot(agent_loc[:,0],agent_loc[:,1], color=colors[idx], marker='*')\n",
    "\n",
    "        ego_translation = self.frames[frame_interval[0]:frame_interval[1]]['ego_translation']\n",
    "        plt.plot(ego_translation[:,0],ego_translation[:,1],label='Ego',color='r')\n",
    "        turn = ' Left Turn' if rotation33_as_yaw(self.frames[frame_interval[0]]['ego_rotation']) - rotation33_as_yaw(self.frames[frame_interval[1]-1]['ego_rotation']) < 0 else ' Right Turn'\n",
    "\n",
    "        plt.axis(\"equal\")\n",
    "        plt.grid(which='both')\n",
    "        plt.legend(fontsize=20,loc='best')\n",
    "        plt.title('Trajectory : Scene '+str(scene)+turn+' at Junction '+junction, fontsize=30)\n",
    "        axes = plt.gca()\n",
    "        \n",
    "#     def agents_state_find(self, scene, output_array, target_agent_type_list):\n",
    "        \n",
    "#         frame_interval = .dataset.scenes[scene]['frame_index_interval']\n",
    "        \n",
    "#         cars_list = target_agent_type_list[0]\n",
    "#         pedestrian_list = target_agent_type_list[1]\n",
    "        \n",
    "#         for agent_list in target_agent_type_list:\n",
    "#             for agent in agent_list:\n",
    "#                     agent_loc = self.agent_centroid[np.where(self.agent_id==agent)[0]]\n",
    "#                     if idx != self.label_dict[target_label]-3 and agent_loc.shape[0] > 30  and np.linalg.norm(agent_loc[0,:]-agent_loc[-1,:]) > 5:\n",
    "#                         for frame in frames\n",
    "\n",
    "#                     elif idx== self.label_dict[target_label]-3:\n",
    "#                         plt.plot(agent_loc[:,0],agent_loc[:,1], color=colors[idx], marker='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = Trajectory(zarr_dataset, Map_Api)\n",
    "Test.generate_info_from_MAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid(intersection_id,grid_boundary):\n",
    "    x = grid_boundary[intersection_id]['X']\n",
    "    y = grid_boundary[intersection_id]['Y']\n",
    "    x_ = np.linspace(x[0], x[1],500)\n",
    "    y_ = np.linspace(y[0], y[1],500)\n",
    "    x_mesh, y_mesh = np.meshgrid(x_, y_)\n",
    "    return x_mesh, y_mesh\n",
    "\n",
    "def locate_in_mesh(x_mesh, y_mesh, point):\n",
    "    x, y = point[0], point[1]\n",
    "    result=[]\n",
    "#     print(x, y, x_mesh[0], x_mesh[-1], y_mesh[0], y_mesh[-1])\n",
    "    if x>x_mesh[-1] or x<x_mesh[0] or y>y_mesh[-1] or y<y_mesh[0]:\n",
    "        return None\n",
    "    result = [math.floor(x-x_mesh[0]), math.floor(y-y_mesh[0])]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replot_from_representation(output[2], lane_list=lane_list, intersection_id='8KfB',  \\\n",
    "                           grid_boundary=grid_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
