{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Flatten, Reshape\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D,concatenate\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import initializers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import math\n",
    "import pickle5 as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "checking gpu error\n",
      "checking GPUs\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:09:00.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu,True)\n",
    "    except:\n",
    "        print('checking gpu error')\n",
    "print('checking GPUs')\n",
    "\n",
    "config=tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.visible_device_list= '0,1'\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "config.log_device_placement=True\n",
    "# config.visible_device_list =2\n",
    "sess=tf.compat.v1.Session(config=config)\n",
    "\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data setting\n",
    "sample_size = 1000\n",
    "num_per_channel=10\n",
    "x_column=25\n",
    "y_column=25\n",
    "channels=3\n",
    "sample_shape= (num_per_channel, x_column, y_column, channels)\n",
    "z_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(img_shape, z_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=z_dim))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dense(num_per_channel*x_column*y_column*channels, activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "    z = Input(shape=(z_dim,))\n",
    "    img = model(z)\n",
    "    return Model(z, img)\n",
    "\n",
    "def discriminator(img_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    img = Input(shape=img_shape)\n",
    "    prediction = model(img)\n",
    "    return Model(img, prediction)\n",
    "\n",
    "discriminator = discriminator(sample_shape)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "generator = generator(sample_shape, z_dim)\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "discriminator.trainable = False\n",
    "prediction = discriminator(img)\n",
    "combined = Model(z, prediction)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 25, 25, 3)\n"
     ]
    }
   ],
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "def train(iterations, batch_size, sample_interval):\n",
    "    gen_images = []\n",
    "    \n",
    "    # Assign X_train to X_train_0 for augment non-cactus images\n",
    "    # Assign X_train to X_train_1 for augment cactus images\n",
    "\n",
    "    X_train = np.random.rand(sample_size, num_per_channel, x_column, y_column, channels)\n",
    "    real = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "       \n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        z = np.random.normal(0, 1, (batch_size, 100))\n",
    "        gen_imgs = generator.predict(z)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, real)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        z = np.random.normal(0, 1, (batch_size, 100))\n",
    "        gen_imgs = generator.predict(z)\n",
    "        g_loss = combined.train_on_batch(z, real)\n",
    "\n",
    "        if iteration % sample_interval == 0:\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (iteration, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            losses.append((d_loss[0], g_loss))\n",
    "            accuracies.append(100*d_loss[1])\n",
    "            gen_images.append(sample_images(iteration))\n",
    "    return gen_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sample_images(iteration, image_grid_rows=4, image_grid_columns=4):\n",
    "\n",
    "    z = np.random.normal(0, 1, \n",
    "              (image_grid_rows * image_grid_columns, z_dim))\n",
    "\n",
    "    gen_imgs = generator.predict(z)\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "            \n",
    "    return gen_imgs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.141338, acc.: 100.00%] [G loss: 1.471242]\n",
      "100 [D loss: 0.143022, acc.: 100.00%] [G loss: 1.520105]\n",
      "200 [D loss: 0.111269, acc.: 100.00%] [G loss: 1.831884]\n",
      "300 [D loss: 0.015712, acc.: 100.00%] [G loss: 3.568152]\n",
      "400 [D loss: 0.016181, acc.: 100.00%] [G loss: 3.658872]\n",
      "500 [D loss: 0.446788, acc.: 50.00%] [G loss: 1.102878]\n",
      "600 [D loss: 0.090484, acc.: 100.00%] [G loss: 1.936445]\n",
      "700 [D loss: 0.075122, acc.: 100.00%] [G loss: 2.139253]\n",
      "800 [D loss: 0.074992, acc.: 100.00%] [G loss: 2.140151]\n",
      "900 [D loss: 0.086324, acc.: 100.00%] [G loss: 2.146873]\n",
      "1000 [D loss: 0.071163, acc.: 100.00%] [G loss: 2.286811]\n",
      "1100 [D loss: 0.016131, acc.: 100.00%] [G loss: 4.007945]\n",
      "1200 [D loss: 0.072102, acc.: 100.00%] [G loss: 2.386014]\n",
      "1300 [D loss: 0.081814, acc.: 100.00%] [G loss: 2.430460]\n",
      "1400 [D loss: 0.023009, acc.: 100.00%] [G loss: 3.133158]\n",
      "1500 [D loss: 0.235325, acc.: 98.44%] [G loss: 1.516053]\n",
      "1600 [D loss: 0.052348, acc.: 100.00%] [G loss: 2.862943]\n",
      "1700 [D loss: 0.022226, acc.: 100.00%] [G loss: 3.794494]\n",
      "1800 [D loss: 0.011536, acc.: 100.00%] [G loss: 4.746554]\n",
      "1900 [D loss: 0.019444, acc.: 100.00%] [G loss: 3.699417]\n",
      "2000 [D loss: 0.120829, acc.: 100.00%] [G loss: 2.403750]\n",
      "2100 [D loss: 0.005133, acc.: 100.00%] [G loss: 5.165123]\n",
      "2200 [D loss: 0.554843, acc.: 48.83%] [G loss: 1.102153]\n",
      "2300 [D loss: 0.334903, acc.: 94.53%] [G loss: 1.548021]\n",
      "2400 [D loss: 0.310772, acc.: 84.38%] [G loss: 3.699985]\n",
      "2500 [D loss: 0.305336, acc.: 100.00%] [G loss: 1.121083]\n",
      "2600 [D loss: 0.217599, acc.: 100.00%] [G loss: 1.418355]\n",
      "2700 [D loss: 0.031645, acc.: 100.00%] [G loss: 3.305145]\n",
      "2800 [D loss: 0.005847, acc.: 100.00%] [G loss: 4.776811]\n",
      "2900 [D loss: 0.332053, acc.: 99.22%] [G loss: 1.316762]\n",
      "3000 [D loss: 0.032695, acc.: 100.00%] [G loss: 3.099142]\n",
      "3100 [D loss: 0.044826, acc.: 100.00%] [G loss: 2.958515]\n",
      "3200 [D loss: 0.214409, acc.: 100.00%] [G loss: 1.358774]\n",
      "3300 [D loss: 0.199273, acc.: 100.00%] [G loss: 1.588078]\n",
      "3400 [D loss: 0.011807, acc.: 100.00%] [G loss: 4.486971]\n",
      "3500 [D loss: 1.551464, acc.: 50.00%] [G loss: 0.339075]\n",
      "3600 [D loss: 0.031571, acc.: 100.00%] [G loss: 4.153370]\n",
      "3700 [D loss: 0.023933, acc.: 100.00%] [G loss: 5.368820]\n",
      "3800 [D loss: 0.321565, acc.: 82.03%] [G loss: 3.698092]\n",
      "3900 [D loss: 0.145202, acc.: 90.62%] [G loss: 3.177629]\n",
      "4000 [D loss: 0.284953, acc.: 91.02%] [G loss: 1.953085]\n",
      "4100 [D loss: 1.006425, acc.: 50.00%] [G loss: 0.453389]\n",
      "4200 [D loss: 0.077013, acc.: 97.27%] [G loss: 3.757359]\n",
      "4300 [D loss: 0.700932, acc.: 62.11%] [G loss: 0.666896]\n",
      "4400 [D loss: 0.373928, acc.: 94.92%] [G loss: 1.247774]\n",
      "4500 [D loss: 0.099895, acc.: 100.00%] [G loss: 3.036471]\n",
      "4600 [D loss: 0.379299, acc.: 84.77%] [G loss: 2.068324]\n",
      "4700 [D loss: 0.557574, acc.: 50.00%] [G loss: 1.534130]\n",
      "4800 [D loss: 0.074219, acc.: 100.00%] [G loss: 2.759124]\n",
      "4900 [D loss: 0.123159, acc.: 100.00%] [G loss: 2.642294]\n"
     ]
    }
   ],
   "source": [
    "# Set iterations at least 10000 for good results\n",
    "iterations = 5000\n",
    "batch_size = 128\n",
    "sample_interval = 100\n",
    "gen_imgs = train(iterations, batch_size, sample_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
