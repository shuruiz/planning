{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "import os\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "import copy as cp\n",
    "import random\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import time\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153031, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "prefix ='/home/lab1/repo/planning/traj_pred/data/'\n",
    "data=None\n",
    "for i in range(3):\n",
    "    name_te= prefix+'veh_test_set_'+str(i)+'.npy'\n",
    "    te = np.load(name_te,allow_pickle=True).astype(float)\n",
    "    if data is None:\n",
    "        data=te\n",
    "    else:\n",
    "        data= np.vstack((data, te))\n",
    "data=data[:,:,:2]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(ele):\n",
    "    \"\"\"\n",
    "    extract feature for a single trajectory\n",
    "    f = [x, y, a,v, delta_a, delta_v]\n",
    "    \"\"\"\n",
    "    fe=[]\n",
    "    for i in range(3, len(ele)):\n",
    "        prev3, prev2, prev, curr = ele[i-3], ele[i-2], ele[i-1], ele[i]\n",
    "        v = np.linalg.norm(curr - prev)/0.5\n",
    "        v_prev = np.linalg.norm(prev - prev2)/0.5\n",
    "        v_prev2 = np.linalg.norm(prev2 - prev3)/0.5\n",
    "        a = (v-v_prev)/0.5\n",
    "        a_prev = (v_prev-v_prev2)/0.5\n",
    "        \n",
    "        energy = v**2-v_prev**2\n",
    "        force = a-a_prev\n",
    "        f = [ele[i][0], ele[i][1], v, a, energy, force]\n",
    "        fe.append(f)\n",
    "    return np.array(fe)\n",
    "\n",
    "def isolation_f(feature):\n",
    "    iso_f =[]\n",
    "    for seq in feature:\n",
    "        iso_f.append(seq[:,3].flatten())\n",
    "    return np.array(iso_f)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153031, 7, 6)\n"
     ]
    }
   ],
   "source": [
    "feature = []\n",
    "for ele in data:\n",
    "    fe= extract_feature(ele)\n",
    "    feature.append(fe)\n",
    "feature=np.array(feature)\n",
    "print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153031, 7)\n"
     ]
    }
   ],
   "source": [
    "iso_feature = isolation_f(feature)\n",
    "print(iso_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 663.27612305 -397.70214844] [ 659.54040527 -393.78775024]\n",
      "5.410924181593378\n",
      "5.410924181593378\n"
     ]
    }
   ],
   "source": [
    "# curr=data[0][0]\n",
    "# prev=data[0][1]\n",
    "# print(curr, prev)\n",
    "# print(math.sqrt((curr[0]-prev[0])**2+(curr[1]-prev[1])**2))\n",
    "# print(np.linalg.norm(curr - prev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(feature, k, metrics=\"dtw\"):\n",
    "\n",
    "    n_jobs=multiprocessing.cpu_count()\n",
    "    print(\"start training clustering on\",n_jobs, \"cpus\")\n",
    "    start_time = time.time()\n",
    "    km_dba = TimeSeriesKMeans(n_clusters=k, metric=metric, max_iter=30, max_iter_barycenter=5, n_jobs=-1, verbose=False, \\\n",
    "                                      random_state=np.random.randint(low=0,high=20,size=2)[0]).fit(feature)\n",
    "    print(\"---used  %s seconds ---\" % (time.time() - start_time))\n",
    "    return km_dba\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(\"risk_model\"):\n",
    "        os.makedirs(\"risk_model\")\n",
    "#     if not os.path.exists(\"saved_centers\"):\n",
    "#         os.makedirs(\"saved_centers\")\n",
    "\n",
    "#     feature = np.load(\"data/Vehicle_trajectory.npy\", allow_pickle=True)\n",
    "    print(feature.shape)\n",
    "\n",
    "    metric=\"dtw\"\n",
    "    n_initial = 10 # how many times to re-initial the run\n",
    "    CENTERS=[]\n",
    "    LABELS=[]\n",
    "    k_min = 6\n",
    "    k_max= 12\n",
    "    elbow = []\n",
    "    \n",
    "    models={}\n",
    "    for k in range(k_min, k_max+1):\n",
    "        print(\"runing number of clusters:\", k)\n",
    "        best_in_a_k=[]\n",
    "        for i in range(0,n_initial):\n",
    "            print(\"running re-initialization:\", i, \"out of\", n_initial)\n",
    "            km_dba = clustering(feature, k, metrics=metric)\n",
    "            CENTERS.append(km_dba.cluster_centers_)\n",
    "            LABELS.append(km_dba.labels_)\n",
    "            best_in_a_k.append(km_dba.inertia_)\n",
    "#             pickle.dump(km_dba, open(\"saved_centers/cluster_model_k\"+str(k)+\"_i\"+str(i)+\".pkl\", 'wb'))\n",
    "        models[k]=km_dba\n",
    "        elbow.append(min(best_in_a_k))\n",
    "    pickle.dump(models, open(\"risk_model/models.pkl\", 'wb'))\n",
    "    pickle.dump(elbow, open(\"risk_model/elbow_list.pkl\", 'wb'))\n",
    "        \n",
    "    plt.plot(range(k_min, k_max+1), elbow)\n",
    "    plt.xlabel('Number of centroids')\n",
    "    plt.ylabel('Elbow score')\n",
    "    plt.title(\"Elbow score on vehicle features\")\n",
    "    plt.savefig(\"risk_model/elbow_plot.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### isolation forest\n",
    "iso_clf = IsolationForest(n_estimators=100, contamination=0.3, behaviour='deprecated', warm_start=True)\n",
    "iso_clf.fit(iso_feature)\n",
    "pickle.dump(clf, open('risk_model/iforest_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############  run below after above is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute prob after finding the best k and its model\n",
    "k_best = 8 # <- manually select\n",
    "kmeans =models[k_best]\n",
    "total = Counter(kmeans.labels_)\n",
    "clustering_labels = kmeans.labels_\n",
    "labels = kmeans.predict(feature)\n",
    "abnormal = defaultdict(int)\n",
    "for i, ele in enumerate(labels):\n",
    "    c=clustering_labels[i]\n",
    "    if ele ==-1:\n",
    "        abnormal[c]+=1\n",
    "prob = defaultdict(float)\n",
    "for n in range(n_cls):\n",
    "    if total[n]==0:\n",
    "        prob[n]=0\n",
    "    else:\n",
    "        prob[n] = round(abnormal[n]/total[n], 3)\n",
    "pickle.dump(prob, open('risk_model/prob.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
